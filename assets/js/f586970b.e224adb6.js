"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9208],{8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const a={},s=r.createContext(a);function t(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),r.createElement(s.Provider,{value:n},e.children)}},9267:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-2-simulation/chapter-4-depth-camera","title":"Depth Camera Integration","description":"Learning Objectives","source":"@site/docs/module-2-simulation/chapter-4-depth-camera.md","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/chapter-4-depth-camera","permalink":"/RoboBook/docs/module-2-simulation/chapter-4-depth-camera","draft":false,"unlisted":false,"editUrl":"https://github.com/NeelamGhazal/RoboBook/tree/main/docs/module-2-simulation/chapter-4-depth-camera.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Depth Camera Integration","sidebar_position":10},"sidebar":"textbookSidebar","previous":{"title":"Chapter 3: IMU Integration","permalink":"/RoboBook/docs/module-2-simulation/chapter-3-imu-integration"},"next":{"title":"Chapter 5: Unity Environment","permalink":"/RoboBook/docs/module-2-simulation/chapter-5-unity-environment"}}');var a=i(4848),s=i(8453);const t={title:"Depth Camera Integration",sidebar_position:10},o="Depth Camera Integration",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Knowledge Prerequisites",id:"knowledge-prerequisites",level:3},{value:"Software Prerequisites",id:"software-prerequisites",level:3},{value:"Installation Verification",id:"installation-verification",level:3},{value:"Introduction",id:"introduction",level:2},{value:"Theory",id:"theory",level:2},{value:"Depth Camera Principles",id:"depth-camera-principles",level:3},{value:"Point Cloud Generation",id:"point-cloud-generation",level:3},{value:"Point Cloud Message Format",id:"point-cloud-message-format",level:3},{value:"Gazebo Depth Camera Simulation",id:"gazebo-depth-camera-simulation",level:3},{value:"Depth Camera Applications",id:"depth-camera-applications",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"URDF with Depth Camera Sensor (robot_with_camera.urdf.xacro)",id:"urdf-with-depth-camera-sensor-robot_with_cameraurdfxacro",level:3},{value:"Depth Camera Processing Node",id:"depth-camera-processing-node",level:3},{value:"Point Cloud Visualization Node",id:"point-cloud-visualization-node",level:3},{value:"Running the Example",id:"running-the-example",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Camera Parameter Tuning",id:"exercise-1-camera-parameter-tuning",level:3},{value:"Exercise 2: Object Detection",id:"exercise-2-object-detection",level:3},{value:"Exercise 3: 3D Mapping",id:"exercise-3-3d-mapping",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"depth-camera-integration",children:"Depth Camera Integration"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Explain the principles of depth cameras and their role in 3D perception"}),"\n",(0,a.jsx)(n.li,{children:"Implement depth camera simulation in Gazebo with realistic parameters"}),"\n",(0,a.jsx)(n.li,{children:"Process depth camera data using sensor_msgs/Image and sensor_msgs/PointCloud2 message types"}),"\n",(0,a.jsx)(n.li,{children:"Generate and visualize 3D point clouds from depth camera data"}),"\n",(0,a.jsx)(n.li,{children:"Integrate depth cameras with robot perception and mapping systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.h3,{id:"knowledge-prerequisites",children:"Knowledge Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Fundamentals"}),": Understanding of nodes, topics, and message types (Module 1)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"URDF Robot Description"}),": Understanding of robot models and sensor integration (Module 1, Chapter 5)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Gazebo Simulation Basics"}),": Understanding of physics simulation concepts (Module 2, Chapter 1)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR and IMU Integration"}),": Understanding of sensor simulation and processing (Module 2, Chapters 2-3)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Computer Vision"}),": Basic understanding of image processing and 3D geometry"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"software-prerequisites",children:"Software Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Operating System"}),": Ubuntu 22.04 LTS with ROS 2 Humble Hawksbill installed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulation Software"}),": Gazebo Garden (or Fortress) with ROS 2 integration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Python"}),": Version 3.10 or higher"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Computer Vision Libraries"}),": OpenCV, NumPy for image processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Point Cloud Library"}),": PCL (Point Cloud Library) for 3D processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visualization Tools"}),": RViz2 for 3D point cloud visualization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Terminal"}),": Bash shell access"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"installation-verification",children:"Installation Verification"}),"\n",(0,a.jsx)(n.p,{children:"Verify your depth camera simulation environment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check available camera topics\nros2 topic list | grep camera\n\n# Check camera message types\nros2 interface show sensor_msgs/msg/Image\nros2 interface show sensor_msgs/msg/CameraInfo\nros2 interface show sensor_msgs/msg/PointCloud2\n\n# Verify Gazebo camera plugins\ngz topic -l | grep camera\n"})}),"\n",(0,a.jsx)(n.p,{children:"Expected output: Available topics, message definitions, and plugins related to camera sensors."}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"In the previous chapters, we explored LiDAR and IMU sensors for environmental perception and self-awareness. Now we'll focus on depth cameras, which provide rich 3D information about the environment by capturing both color (RGB) and depth (D) data. Depth cameras combine the visual richness of traditional cameras with the geometric accuracy of distance measurements, creating RGB-D sensors that provide detailed 3D representations of the environment."}),"\n",(0,a.jsx)(n.p,{children:'Think of a depth camera as a robot\'s "3D vision system" - while traditional cameras provide 2D color information, depth cameras add the crucial third dimension, enabling robots to perceive depth, volume, and spatial relationships. This capability is essential for tasks like object recognition, scene understanding, and 3D mapping that require more than simple distance measurements.'}),"\n",(0,a.jsx)(n.p,{children:"In Physical AI systems, depth cameras are particularly valuable because they provide dense 3D information that enables detailed scene understanding. Unlike LiDAR, which provides sparse distance measurements, depth cameras generate dense depth maps with millions of depth values per frame. This makes them ideal for applications requiring detailed object recognition, surface analysis, and complex scene understanding."}),"\n",(0,a.jsx)(n.p,{children:"In this chapter, we'll explore how to simulate depth cameras in Gazebo, configure realistic sensor parameters, and process the resulting RGB-D data in ROS 2. We'll learn to convert depth images to 3D point clouds and integrate this information into robot perception systems."}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-principles",children:"Depth Camera Principles"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras work using various technologies:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Stereo Vision"}),": Two cameras capture images from slightly different positions, using triangulation to calculate depth"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Time-of-Flight (ToF)"}),": Measures the time light takes to travel to objects and back"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Structured Light"}),": Projects known light patterns and analyzes distortions to calculate depth"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The data from depth cameras includes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"RGB Image"}),": Color information (sensor_msgs/Image with 8UC3 encoding)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Image"}),": Distance information (sensor_msgs/Image with 16UC1 or 32FC1 encoding)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Camera Info"}),": Intrinsic parameters (sensor_msgs/CameraInfo)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"point-cloud-generation",children:"Point Cloud Generation"}),"\n",(0,a.jsx)(n.p,{children:"Depth images can be converted to 3D point clouds using the camera's intrinsic parameters:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"x = (u - cx) * depth / fx\ny = (v - cy) * depth / fy\nz = depth\n"})}),"\n",(0,a.jsx)(n.p,{children:"Where:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"(u, v) are pixel coordinates in the image"}),"\n",(0,a.jsx)(n.li,{children:"(cx, cy) are the principal point coordinates"}),"\n",(0,a.jsx)(n.li,{children:"(fx, fy) are the focal lengths in pixels"}),"\n",(0,a.jsx)(n.li,{children:"depth is the distance value at that pixel"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"point-cloud-message-format",children:"Point Cloud Message Format"}),"\n",(0,a.jsx)(n.p,{children:"The sensor_msgs/PointCloud2 message contains:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"header"}),": Timestamp and frame information"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"height"}),", ",(0,a.jsx)(n.code,{children:"width"}),": Dimensions of the point cloud"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"fields"}),": Definition of data fields (x, y, z, rgb, etc.)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"is_bigendian"}),": Endianness of data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"point_step"}),": Size of each point in bytes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"row_step"}),": Size of each row in bytes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"data"}),": Raw binary data containing point coordinates and attributes"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"gazebo-depth-camera-simulation",children:"Gazebo Depth Camera Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo simulates depth cameras using ray tracing:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:'%%{title: "Gazebo Depth Camera Simulation Pipeline"}%%\ngraph TD\n    A[Depth Camera Model] --\x3e B[Ray Tracing Engine]\n    B --\x3e C[RGB Image Generation]\n    C --\x3e D[Depth Image Generation]\n    D --\x3e E[Point Cloud Conversion]\n    E --\x3e F[ROS 2 Topics: /camera/image, /camera/depth, /camera/points]\n    F --\x3e G[Perception Algorithms]\n'})}),"\n",(0,a.jsx)(n.p,{children:"The simulation accurately models:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Camera intrinsic parameters (focal length, principal point)"}),"\n",(0,a.jsx)(n.li,{children:"Depth accuracy and noise characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Field of view and resolution"}),"\n",(0,a.jsx)(n.li,{children:"Coordinate frame transformations"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-applications",children:"Depth Camera Applications"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras enable several critical robot capabilities:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Object Recognition"}),": Identify and classify objects in 3D space"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scene Reconstruction"}),": Build detailed 3D models of environments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Grasping and Manipulation"}),": Enable precise object interaction"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"3D Mapping"}),": Create dense 3D maps for navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human-Robot Interaction"}),": Enable gesture recognition and spatial awareness"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(n.p,{children:"Let's implement a complete depth camera integration example:"}),"\n",(0,a.jsx)(n.h3,{id:"urdf-with-depth-camera-sensor-robot_with_cameraurdfxacro",children:"URDF with Depth Camera Sensor (robot_with_camera.urdf.xacro)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="robot_with_camera">\n\n  \x3c!-- Constants --\x3e\n  <xacro:property name="M_PI" value="3.1415926535897931" />\n\n  \x3c!-- Robot base properties --\x3e\n  <xacro:property name="base_width" value="0.4" />\n  <xacro:property name="base_length" value="0.6" />\n  <xacro:property name="base_height" value="0.2" />\n  <xacro:property name="base_mass" value="10.0" />\n\n  \x3c!-- Wheel properties --\x3e\n  <xacro:property name="wheel_radius" value="0.1" />\n  <xacro:property name="wheel_width" value="0.05" />\n  <xacro:property name="wheel_mass" value="1.0" />\n  <xacro:property name="wheel_offset_x" value="0.2" />\n  <xacro:property name="wheel_offset_y" value="0.25" />\n  <xacro:property name="wheel_offset_z" value="-0.05" />\n\n  \x3c!-- Camera properties --\x3e\n  <xacro:property name="camera_size" value="0.02" />\n  <xacro:property name="camera_mass" value="0.05" />\n\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${base_length} ${base_width} ${base_height}"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${base_length} ${base_width} ${base_height}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${base_mass}"/>\n      <inertia\n        ixx="${base_mass/12.0 * (base_width*base_width + base_height*base_height)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${base_mass/12.0 * (base_length*base_length + base_height*base_height)}"\n        iyz="0.0"\n        izz="${base_mass/12.0 * (base_length*base_length + base_width*base_width)}" />\n    </inertial>\n  </link>\n\n  \x3c!-- Left wheel --\x3e\n  <link name="left_wheel">\n    <visual>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${wheel_mass}"/>\n      <inertia\n        ixx="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        iyz="0.0"\n        izz="${wheel_mass/2.0 * wheel_radius*wheel_radius}" />\n    </inertial>\n  </link>\n\n  <joint name="left_wheel_joint" type="continuous">\n    <origin xyz="${wheel_offset_x} ${wheel_offset_y} ${wheel_offset_z}" rpy="0 0 0"/>\n    <parent link="base_link"/>\n    <child link="left_wheel"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  \x3c!-- Right wheel --\x3e\n  <link name="right_wheel">\n    <visual>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${wheel_mass}"/>\n      <inertia\n        ixx="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        iyz="0.0"\n        izz="${wheel_mass/2.0 * wheel_radius*wheel_radius}" />\n    </inertial>\n  </link>\n\n  <joint name="right_wheel_joint" type="continuous">\n    <origin xyz="${wheel_offset_x} ${-wheel_offset_y} ${wheel_offset_z}" rpy="0 0 0"/>\n    <parent link="base_link"/>\n    <child link="right_wheel"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  \x3c!-- RGB-D Camera --\x3e\n  <link name="camera_link">\n    <visual>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${camera_size} ${camera_size*1.5} ${camera_size}"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${camera_size} ${camera_size*1.5} ${camera_size}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${camera_mass}"/>\n      <inertia\n        ixx="${camera_mass/12.0 * (camera_size*camera_size + camera_size*camera_size)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${camera_mass/12.0 * (camera_size*camera_size + camera_size*camera_size)}"\n        iyz="0.0"\n        izz="${camera_mass/12.0 * (camera_size*camera_size + camera_size*camera_size)}" />\n    </inertial>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <origin xyz="${base_length/2 - camera_size/2} 0 ${base_height/2 + camera_size/2 + 0.01}" rpy="0 0 0"/>\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n  </joint>\n\n  \x3c!-- Gazebo plugin for RGB-D camera --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="depth_camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.089</horizontal_fov> \x3c!-- 62.4 degrees --\x3e\n        <image>\n          <format>R8G8B8</format>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10.0</far>\n        </clip>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.007</stddev>\n        </noise>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n        <always_on>true</always_on>\n        <update_rate>30</update_rate>\n        <camera_name>camera</camera_name>\n        <frame_name>camera_link</frame_name>\n        <baseline>0.1</baseline>\n        <distortion_k1>0.0</distortion_k1>\n        <distortion_k2>0.0</distortion_k2>\n        <distortion_k3>0.0</distortion_k3>\n        <distortion_t1>0.0</distortion_t1>\n        <distortion_t2>0.0</distortion_t2>\n        <point_cloud_cutoff>0.1</point_cloud_cutoff>\n        <point_cloud_cutoff_max>10.0</point_cloud_cutoff_max>\n        <Cx_prime>0</Cx_prime>\n        <Cx>320.5</Cx>\n        <Cy>240.5</Cy>\n        <focal_length>320.0</focal_length>\n        <hack_baseline>0.0</hack_baseline>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-processing-node",children:"Depth Camera Processing Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\nfrom sensor_msgs_py import point_cloud2\nfrom sensor_msgs.msg import PointField\nimport struct\n\n\nclass DepthCameraProcessor(Node):\n    \"\"\"\n    Node that processes depth camera data to generate point clouds and extract features.\n    Demonstrates RGB-D processing techniques in ROS 2.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('depth_camera_processor')\n\n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n\n        # Create subscribers for RGB and depth images\n        self.rgb_sub = Subscriber(self, Image, '/camera/image')\n        self.depth_sub = Subscriber(self, Image, '/camera/depth/image_raw')\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publishers\n        self.pointcloud_pub = self.create_publisher(\n            PointCloud2,\n            '/camera/depth/points',\n            10\n        )\n\n        self.processed_rgb_pub = self.create_publisher(\n            Image,\n            '/camera/image_processed',\n            10\n        )\n\n        # Synchronize RGB and depth image timestamps\n        self.ts = ApproximateTimeSynchronizer(\n            [self.rgb_sub, self.depth_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        self.ts.registerCallback(self.image_callback)\n\n        # Camera parameters (will be updated from camera_info)\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        self.get_logger().info('Depth camera processor initialized')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Update camera parameters from camera info.\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, rgb_msg, depth_msg):\n        \"\"\"Process synchronized RGB and depth images.\"\"\"\n        try:\n            # Convert ROS images to OpenCV format\n            rgb_image = self.bridge.imgmsg_to_cv2(rgb_msg, 'bgr8')\n            depth_image = self.bridge.imgmsg_to_cv2(depth_msg, '32FC1')\n\n            # Process RGB image (example: edge detection)\n            gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n\n            # Publish processed RGB image\n            processed_msg = self.bridge.cv2_to_imgmsg(edges, 'mono8')\n            processed_msg.header = rgb_msg.header\n            self.processed_rgb_pub.publish(processed_msg)\n\n            # Generate point cloud from depth image\n            pointcloud_msg = self.depth_to_pointcloud(depth_image, rgb_image, rgb_msg.header)\n            if pointcloud_msg is not None:\n                self.pointcloud_pub.publish(pointcloud_msg)\n\n            # Log information about the processed frame\n            valid_depths = np.count_nonzero(~np.isnan(depth_image) & ~np.isinf(depth_image))\n            self.get_logger().info(f'Processed frame: {valid_depths} valid depth points')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing images: {e}')\n\n    def depth_to_pointcloud(self, depth_image, rgb_image, header):\n        \"\"\"Convert depth image to point cloud.\"\"\"\n        if self.camera_matrix is None:\n            return None\n\n        height, width = depth_image.shape\n        points = []\n\n        # Get camera intrinsic parameters\n        fx = self.camera_matrix[0, 0]\n        fy = self.camera_matrix[1, 1]\n        cx = self.camera_matrix[0, 2]\n        cy = self.camera_matrix[1, 2]\n\n        # Generate points for each pixel\n        for v in range(height):\n            for u in range(width):\n                depth = depth_image[v, u]\n\n                # Skip invalid depth values\n                if np.isnan(depth) or np.isinf(depth) or depth <= 0:\n                    continue\n\n                # Convert pixel coordinates to 3D world coordinates\n                x = (u - cx) * depth / fx\n                y = (v - cy) * depth / fy\n                z = depth\n\n                # Get color from RGB image\n                if u < rgb_image.shape[1] and v < rgb_image.shape[0]:\n                    b, g, r = rgb_image[v, u]\n                else:\n                    r, g, b = 0, 0, 0\n\n                # Pack color as single float (for PointCloud2)\n                rgb = struct.unpack('I', struct.pack('BBBB', b, g, r, 255))[0]\n\n                points.append([x, y, z, rgb])\n\n        if not points:\n            return None\n\n        # Create PointCloud2 message\n        fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\n            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)\n        ]\n\n        # Create the PointCloud2 message\n        pointcloud_msg = PointCloud2()\n        pointcloud_msg.header = header\n        pointcloud_msg.height = 1\n        pointcloud_msg.width = len(points)\n        pointcloud_msg.fields = fields\n        pointcloud_msg.is_bigendian = False\n        pointcloud_msg.is_dense = False\n        pointcloud_msg.point_step = 16  # 3*4 bytes for xyz + 4 bytes for rgb\n        pointcloud_msg.row_step = pointcloud_msg.point_step * pointcloud_msg.width\n\n        # Pack the points into binary data\n        data = []\n        for point in points:\n            data.append(struct.pack('fffI', point[0], point[1], point[2], int(point[3])))\n\n        # Flatten the binary data\n        binary_data = b''.join(data)\n        pointcloud_msg.data = binary_data\n\n        return pointcloud_msg\n\n\ndef main(args=None):\n    \"\"\"Main function to run the depth camera processor.\"\"\"\n    rclpy.init(args=args)\n\n    processor = DepthCameraProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info('Interrupt received, shutting down...')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"point-cloud-visualization-node",children:"Point Cloud Visualization Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2\nfrom std_msgs.msg import Header\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport struct\nfrom sensor_msgs_py import point_cloud2\n\n\nclass PointCloudVisualizer(Node):\n    """\n    Node that visualizes point cloud data in RViz2.\n    Converts PointCloud2 messages to visualization markers.\n    """\n\n    def __init__(self):\n        super().__init__(\'pointcloud_visualizer\')\n\n        # Create subscriber for point cloud data\n        self.pc_subscriber = self.create_subscription(\n            PointCloud2,\n            \'/camera/depth/points\',\n            self.pointcloud_callback,\n            10\n        )\n\n        # Create publisher for visualization markers\n        self.marker_publisher = self.create_publisher(\n            Marker,\n            \'/pointcloud_visualization\',\n            10\n        )\n\n        self.get_logger().info(\'Point cloud visualizer initialized\')\n\n    def pointcloud_callback(self, msg):\n        """Convert PointCloud2 to visualization markers."""\n        try:\n            # Create marker for point cloud visualization\n            marker = Marker()\n            marker.header = msg.header\n            marker.ns = "pointcloud"\n            marker.id = 0\n            marker.type = Marker.POINTS\n            marker.action = Marker.ADD\n\n            # Set marker scale (adjust based on expected point cloud density)\n            marker.scale.x = 0.01  # Point width\n            marker.scale.y = 0.01  # Point height\n            marker.scale.z = 0.01  # Point depth\n\n            # Set default color (will be overridden by point colors if available)\n            marker.color.r = 1.0\n            marker.color.g = 1.0\n            marker.color.b = 1.0\n            marker.color.a = 1.0\n\n            # Extract points from PointCloud2 message\n            points = []\n            for point in point_cloud2.read_points(msg, field_names=("x", "y", "z", "rgb"), skip_nans=True):\n                x, y, z = point[0], point[1], point[2]\n\n                # Create a point\n                p = Point()\n                p.x = x\n                p.y = y\n                p.z = z\n                points.append(p)\n\n            marker.points = points\n\n            # Publish the marker\n            self.marker_publisher.publish(marker)\n\n            self.get_logger().info(f\'Published visualization for {len(points)} points\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error visualizing point cloud: {e}\')\n\n\ndef main(args=None):\n    """Main function to run the point cloud visualizer."""\n    rclpy.init(args=args)\n\n    visualizer = PointCloudVisualizer()\n\n    try:\n        rclpy.spin(visualizer)\n    except KeyboardInterrupt:\n        visualizer.get_logger().info(\'Interrupt received, shutting down...\')\n    finally:\n        visualizer.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"[INFO] [depth_camera_processor]: Depth camera processor initialized\n[INFO] [depth_camera_processor]: Processed frame: 12500 valid depth points\n[INFO] [pointcloud_visualizer]: Published visualization for 12500 points\n[INFO] [depth_camera_processor]: Interrupt received, shutting down...\n"})}),"\n",(0,a.jsx)(n.h3,{id:"running-the-example",children:"Running the Example"}),"\n",(0,a.jsx)(n.p,{children:"To run this depth camera integration example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start Gazebo with robot model\nsource /opt/ros/humble/setup.bash\ngz sim -r simple_world.sdf\n\n# Terminal 2: Spawn the robot with depth camera in Gazebo\nsource /opt/ros/humble/setup.bash\nros2 run gazebo_ros spawn_entity.py -topic robot_description -entity my_robot\n\n# Terminal 3: Run the depth camera processor\nsource /opt/ros/humble/setup.bash\nros2 run my_package depth_camera_processor\n\n# Terminal 4: Run the point cloud visualizer\nsource /opt/ros/humble/setup.bash\nros2 run my_package pointcloud_visualizer\n\n# Terminal 5: Monitor camera topics\nsource /opt/ros/humble/setup.bash\nros2 topic echo /camera/image --field data --field header.stamp.sec\nros2 topic echo /camera/depth/image_raw --field data --field header.stamp.sec\n\n# Terminal 6: Visualize in RViz2\nsource /opt/ros/humble/setup.bash\nrviz2\n# In RViz2: Add Image display for /camera/image, PointCloud2 display for /camera/depth/points\n"})}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-1-camera-parameter-tuning",children:"Exercise 1: Camera Parameter Tuning"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Experiment with different depth camera parameters in Gazebo."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Modify the URDF to change camera resolution, field of view, and depth range"}),"\n",(0,a.jsx)(n.li,{children:"Compare point cloud quality and density with different settings"}),"\n",(0,a.jsx)(n.li,{children:"Document the trade-offs between quality and computational cost"}),"\n",(0,a.jsx)(n.li,{children:"Test how different parameters affect 3D reconstruction quality"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Different camera configurations are properly implemented"}),"\n",(0,a.jsx)(n.li,{children:"Quality differences are documented"}),"\n",(0,a.jsx)(n.li,{children:"Trade-offs between parameters are understood"}),"\n",(0,a.jsx)(n.li,{children:"Optimal settings for specific applications are identified"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-2-object-detection",children:"Exercise 2: Object Detection"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Implement object detection using RGB-D data."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Create a node that segments objects from the background using depth information"}),"\n",(0,a.jsx)(n.li,{children:"Implement color-based segmentation to identify specific objects"}),"\n",(0,a.jsx)(n.li,{children:"Calculate 3D bounding boxes for detected objects"}),"\n",(0,a.jsx)(n.li,{children:"Test the algorithm with different objects in the scene"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Object segmentation works correctly using RGB-D data"}),"\n",(0,a.jsx)(n.li,{children:"3D bounding boxes are calculated accurately"}),"\n",(0,a.jsx)(n.li,{children:"Algorithm performs well with various objects"}),"\n",(0,a.jsx)(n.li,{children:"Detection results are properly visualized"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-3-3d-mapping",children:"Exercise 3: 3D Mapping"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Create a 3D map using depth camera data."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement a simple 3D mapping algorithm that accumulates point clouds"}),"\n",(0,a.jsx)(n.li,{children:"Use odometry data to transform points to a global coordinate frame"}),"\n",(0,a.jsx)(n.li,{children:"Create a 3D occupancy grid or mesh from the accumulated points"}),"\n",(0,a.jsx)(n.li,{children:"Visualize the resulting 3D map in RViz2"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"3D mapping algorithm properly accumulates point clouds"}),"\n",(0,a.jsx)(n.li,{children:"Coordinate transformations are correctly applied"}),"\n",(0,a.jsx)(n.li,{children:"3D map is generated and visualized"}),"\n",(0,a.jsx)(n.li,{children:"Mapping quality improves over time as more data is collected"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras provide rich 3D perception capabilities by combining color and depth information, enabling robots to understand their environment in three dimensions. We've explored how to integrate depth cameras in Gazebo simulation, configure realistic parameters, and process the resulting RGB-D data in ROS 2. The combination of sensor_msgs/Image for color and depth data and sensor_msgs/PointCloud2 for 3D representations provides a complete interface for 3D perception applications."}),"\n",(0,a.jsx)(n.p,{children:"We've implemented complete examples showing depth camera integration in URDF models, RGB-D processing nodes for point cloud generation, and visualization techniques for 3D data. The examples demonstrated how to convert depth images to 3D point clouds and extract meaningful information for robot perception tasks."}),"\n",(0,a.jsx)(n.p,{children:"Understanding depth camera integration is crucial for Physical AI systems that require detailed 3D scene understanding. The dense geometric and color information provided by depth cameras enables advanced applications like object recognition, scene reconstruction, and precise manipulation that are difficult or impossible with other sensor types."}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"Now that you understand depth camera integration, the next chapter explores Unity simulation environments. You'll learn how to create high-fidelity visual simulations with realistic rendering for perception tasks."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Next Chapter"}),": Module 2, Chapter 5: Unity Simulation Environment"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);