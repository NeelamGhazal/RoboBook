"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4341],{5817:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2-simulation/chapter-5-unity-environment","title":"Unity Simulation Environment","description":"Learning Objectives","source":"@site/docs/module-2-simulation/chapter-5-unity-environment.md","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/chapter-5-unity-environment","permalink":"/RoboBook/docs/module-2-simulation/chapter-5-unity-environment","draft":false,"unlisted":false,"editUrl":"https://github.com/NeelamGhazal/RoboBook/tree/main/docs/module-2-simulation/chapter-5-unity-environment.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Unity Simulation Environment","sidebar_position":11},"sidebar":"textbookSidebar","previous":{"title":"Chapter 4: Depth Camera Integration","permalink":"/RoboBook/docs/module-2-simulation/chapter-4-depth-camera"},"next":{"title":"Chapter 1: Isaac Overview","permalink":"/RoboBook/docs/module-3-isaac/chapter-1-isaac-overview"}}');var r=i(4848),s=i(8453);const o={title:"Unity Simulation Environment",sidebar_position:11},a="Unity Simulation Environment",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Knowledge Prerequisites",id:"knowledge-prerequisites",level:3},{value:"Software Prerequisites",id:"software-prerequisites",level:3},{value:"Installation Verification",id:"installation-verification",level:3},{value:"Introduction",id:"introduction",level:2},{value:"Theory",id:"theory",level:2},{value:"Unity Robotics Ecosystem",id:"unity-robotics-ecosystem",level:3},{value:"Unity vs. Gazebo Comparison",id:"unity-vs-gazebo-comparison",level:3},{value:"Unity Simulation Architecture",id:"unity-simulation-architecture",level:3},{value:"Perception-Specific Features",id:"perception-specific-features",level:3},{value:"When to Use Unity vs. Gazebo",id:"when-to-use-unity-vs-gazebo",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Unity C# Script for ROS Communication (RobotController.cs)",id:"unity-c-script-for-ros-communication-robotcontrollercs",level:3},{value:"Unity C# Script for Camera Integration (CameraPublisher.cs)",id:"unity-c-script-for-camera-integration-camerapublishercs",level:3},{value:"Python ROS 2 Node to Interface with Unity",id:"python-ros-2-node-to-interface-with-unity",level:3},{value:"Unity Scene Setup Instructions",id:"unity-scene-setup-instructions",level:3},{value:"Running the Example",id:"running-the-example",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Unity Environment Creation",id:"exercise-1-unity-environment-creation",level:3},{value:"Exercise 2: Advanced Sensor Simulation",id:"exercise-2-advanced-sensor-simulation",level:3},{value:"Exercise 3: Perception Training Data",id:"exercise-3-perception-training-data",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"unity-simulation-environment",children:"Unity Simulation Environment"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain the Unity ecosystem and its role in high-fidelity robotics simulation"}),"\n",(0,r.jsx)(n.li,{children:"Set up Unity Robotics Simulation environment with ROS-TCP-Connector"}),"\n",(0,r.jsx)(n.li,{children:"Create realistic 3D environments with detailed lighting and materials"}),"\n",(0,r.jsx)(n.li,{children:"Integrate Unity with ROS 2 for bidirectional communication"}),"\n",(0,r.jsx)(n.li,{children:"Compare Unity vs. Gazebo for different robotics applications"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"knowledge-prerequisites",children:"Knowledge Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Fundamentals"}),": Understanding of nodes, topics, and message types (Module 1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo Simulation"}),": Understanding of physics-based simulation concepts (Module 2, Chapters 1-4)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URDF Robot Description"}),": Understanding of robot models and sensor integration (Module 1, Chapter 5)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computer Graphics"}),": Basic understanding of 3D rendering, lighting, and materials"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"C# Programming"}),": Basic understanding of C# for Unity scripting (or willingness to learn)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"software-prerequisites",children:"Software Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Hub"}),": Unity 2021.3 LTS or later with Unity Editor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Robotics Package"}),": Unity Robotics Hub and ROS-TCP-Connector"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2"}),": Humble Hawksbill with necessary packages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Python"}),": Version 3.10 or higher"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Operating System"}),": Ubuntu 22.04 LTS (Linux support) or Windows 10/11"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": GPU with DirectX 11 or OpenGL 4.1+ support, 8GB+ RAM recommended"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"installation-verification",children:"Installation Verification"}),"\n",(0,r.jsx)(n.p,{children:"Verify your Unity robotics environment:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Check if Unity Robotics packages are installed\n# In Unity Editor: Window \u2192 Package Manager \u2192 Check for "ROS TCP Connector"\n\n# Verify ROS 2 connection capability\nros2 pkg list | grep unity\n# Look for unity_robotics_* packages if installed via ROS ecosystem\n\n# Check network connectivity for TCP communication\nnetstat -tuln | grep 10000\n# Unity ROS TCP connector typically uses port 10000\n'})}),"\n",(0,r.jsx)(n.p,{children:"Expected output: Unity packages available and network port accessible."}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"In the previous chapters, we explored Gazebo for physics-accurate simulation and various sensor integrations. Now we'll focus on Unity, which offers a different approach to robotics simulation: high-fidelity graphics and realistic rendering for perception tasks. While Gazebo excels at physics simulation, Unity excels at creating photorealistic environments that are essential for training computer vision algorithms and testing perception systems."}),"\n",(0,r.jsx)(n.p,{children:'Think of Unity as a "movie studio" for robots - just as filmmakers create realistic visual environments for movies, Unity creates photorealistic worlds where robots can practice perception tasks. This is particularly valuable for training deep learning models that need to recognize objects, navigate complex scenes, or interact with humans in realistic environments. Unity\'s advanced rendering pipeline can simulate lighting conditions, materials, and visual effects that are difficult to achieve in physics-focused simulators.'}),"\n",(0,r.jsx)(n.p,{children:"In Physical AI systems, Unity simulation is particularly valuable for perception-heavy applications: training object detection models, testing visual SLAM algorithms, creating synthetic datasets, and developing human-robot interaction scenarios. The combination of realistic graphics with the ability to generate ground truth data (exact object positions, depths, and classifications) makes Unity an invaluable tool for perception system development."}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, we'll explore how to set up Unity for robotics simulation, create realistic environments, and integrate with ROS 2 for bidirectional communication. We'll learn when to use Unity versus Gazebo based on application requirements."}),"\n",(0,r.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,r.jsx)(n.h3,{id:"unity-robotics-ecosystem",children:"Unity Robotics Ecosystem"}),"\n",(0,r.jsx)(n.p,{children:"Unity provides several tools for robotics simulation:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Robotics Hub"}),": Central package for robotics development"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS-TCP-Connector"}),": Bridge between Unity and ROS 2 via TCP"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Perception Package"}),": Tools for generating synthetic training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity ML-Agents"}),": Framework for training AI agents in Unity"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS#"}),": Alternative ROS bridge for Unity"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"unity-vs-gazebo-comparison",children:"Unity vs. Gazebo Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Unity"}),(0,r.jsx)(n.th,{children:"Gazebo"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Rendering"})}),(0,r.jsx)(n.td,{children:"Photorealistic, RTX-capable"}),(0,r.jsx)(n.td,{children:"Basic visualization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Physics"})}),(0,r.jsx)(n.td,{children:"Good (PhysX engine)"}),(0,r.jsx)(n.td,{children:"Excellent (ODE, DART, Bullet)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Perception"})}),(0,r.jsx)(n.td,{children:"Excellent for vision tasks"}),(0,r.jsx)(n.td,{children:"Basic sensor models"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Development"})}),(0,r.jsx)(n.td,{children:"C# scripting, visual editor"}),(0,r.jsx)(n.td,{children:"C++/Python, SDF/URDF"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Performance"})}),(0,r.jsx)(n.td,{children:"GPU-intensive"}),(0,r.jsx)(n.td,{children:"CPU-focused"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Use Case"})}),(0,r.jsx)(n.td,{children:"Vision, perception, graphics"}),(0,r.jsx)(n.td,{children:"Control, dynamics, sensors"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"unity-simulation-architecture",children:"Unity Simulation Architecture"}),"\n",(0,r.jsx)(n.p,{children:"The Unity-ROS communication follows a TCP-based architecture:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:'%%{title: "Unity-ROS Communication Architecture"}%%\ngraph LR\n    A[Unity Scene] --\x3e B[ROS TCP Connector]\n    B --\x3e C[TCP/IP Network]\n    C --\x3e D[ROS 2 Bridge]\n    D --\x3e E[ROS 2 Nodes]\n    E --\x3e F[Algorithms & Applications]\n\n    F --\x3e E\n    E --\x3e D\n    D --\x3e C\n    C --\x3e B\n    B --\x3e A\n'})}),"\n",(0,r.jsx)(n.p,{children:"This architecture enables:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time bidirectional communication"}),"\n",(0,r.jsx)(n.li,{children:"Support for all ROS 2 message types"}),"\n",(0,r.jsx)(n.li,{children:"Integration with existing ROS 2 tools"}),"\n",(0,r.jsx)(n.li,{children:"Distributed simulation across networks"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"perception-specific-features",children:"Perception-Specific Features"}),"\n",(0,r.jsx)(n.p,{children:"Unity excels at perception tasks with features like:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Realistic Materials"}),": Physically-based rendering (PBR) materials"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advanced Lighting"}),": Global illumination, reflections, shadows"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Simulation"}),": RGB, depth, semantic segmentation cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization"}),": Automatic variation of visual properties"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Ground truth annotations for training"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-unity-vs-gazebo",children:"When to Use Unity vs. Gazebo"}),"\n",(0,r.jsx)(n.p,{children:"Use Unity when:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Training computer vision models"}),"\n",(0,r.jsx)(n.li,{children:"Testing perception algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Creating photorealistic environments"}),"\n",(0,r.jsx)(n.li,{children:"Developing human-robot interaction"}),"\n",(0,r.jsx)(n.li,{children:"Needing advanced graphics capabilities"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Use Gazebo when:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Testing control algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Simulating complex physics"}),"\n",(0,r.jsx)(n.li,{children:"Working with accurate sensor models"}),"\n",(0,r.jsx)(n.li,{children:"Needing real-time physics simulation"}),"\n",(0,r.jsx)(n.li,{children:"Integrating with existing Gazebo ecosystems"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.p,{children:"Let's implement a complete Unity simulation integration example:"}),"\n",(0,r.jsx)(n.h3,{id:"unity-c-script-for-ros-communication-robotcontrollercs",children:"Unity C# Script for ROS Communication (RobotController.cs)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageGeneration;\nusing RosMessageTypes.Geometry;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Std;\n\npublic class RobotController : MonoBehaviour\n{\n    // ROS Connector reference\n    private ROSConnection ros;\n\n    // Topic names\n    [SerializeField] private string cmdVelTopic = "/cmd_vel";\n    [SerializeField] private string odomTopic = "/odom";\n    [SerializeField] private string scanTopic = "/scan";\n\n    // Robot properties\n    [SerializeField] private float maxLinearVelocity = 1.0f;\n    [SerializeField] private float maxAngularVelocity = 1.0f;\n    [SerializeField] private float wheelRadius = 0.1f;\n    [SerializeField] private float wheelBase = 0.5f;\n\n    // Robot state\n    private float linearVelocity = 0.0f;\n    private float angularVelocity = 0.0f;\n\n    // Previous state for odometry\n    private Vector3 previousPosition;\n    private Quaternion previousRotation;\n    private float timePreviousUpdate;\n\n    void Start()\n    {\n        // Get the ROS connection static instance\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Subscribe to command velocity topic\n        ros.Subscribe<TwistMsg>(cmdVelTopic, CmdVelCallback);\n\n        // Initialize state\n        previousPosition = transform.position;\n        previousRotation = transform.rotation;\n        timePreviousUpdate = Time.time;\n    }\n\n    void Update()\n    {\n        // Update robot position based on velocities\n        UpdateRobotPosition();\n\n        // Publish odometry periodically\n        if (Time.time - timePreviousUpdate > 0.1f) // 10Hz\n        {\n            PublishOdometry();\n            timePreviousUpdate = Time.time;\n        }\n    }\n\n    void CmdVelCallback(TwistMsg cmd)\n    {\n        // Process incoming velocity commands\n        linearVelocity = Mathf.Clamp((float)cmd.linear.x, -maxLinearVelocity, maxLinearVelocity);\n        angularVelocity = Mathf.Clamp((float)cmd.angular.z, -maxAngularVelocity, maxAngularVelocity);\n    }\n\n    void UpdateRobotPosition()\n    {\n        // Simple differential drive kinematics\n        float deltaTime = Time.deltaTime;\n\n        // Calculate new position and rotation\n        Vector3 forward = transform.forward;\n        Vector3 newPosition = transform.position + forward * linearVelocity * deltaTime;\n\n        float rotationAmount = angularVelocity * deltaTime;\n        Quaternion newRotation = transform.rotation * Quaternion.Euler(0, rotationAmount * Mathf.Rad2Deg, 0);\n\n        // Apply new position and rotation\n        transform.position = newPosition;\n        transform.rotation = newRotation;\n    }\n\n    void PublishOdometry()\n    {\n        // Calculate odometry data\n        float deltaTime = Time.time - timePreviousUpdate;\n\n        // Calculate velocities based on position change\n        Vector3 positionChange = transform.position - previousPosition;\n        Vector3 velocity = positionChange / deltaTime;\n\n        // Calculate angular velocity\n        float angleChange = Quaternion.Angle(previousRotation, transform.rotation);\n        float angularVel = angleChange * Mathf.Deg2Rad / deltaTime;\n\n        // Create odometry message\n        var odomMsg = new OdometryMsg();\n        odomMsg.header = new std_msgs.Header();\n        odomMsg.header.stamp = new builtin_interfaces.Time();\n        odomMsg.header.frame_id = "odom";\n\n        // Set position\n        odomMsg.pose.pose.position = new geometry_msgs.Point(transform.position.x, transform.position.y, transform.position.z);\n        odomMsg.pose.pose.orientation = new geometry_msgs.Quaternion(transform.rotation.x, transform.rotation.y, transform.rotation.z, transform.rotation.w);\n\n        // Set velocities\n        odomMsg.twist.twist.linear = new geometry_msgs.Vector3(velocity.x, velocity.y, velocity.z);\n        odomMsg.twist.twist.angular = new geometry_msgs.Vector3(0, 0, angularVel);\n\n        // Publish odometry message\n        ros.Send(odomTopic, odomMsg);\n\n        // Update previous state\n        previousPosition = transform.position;\n        previousRotation = transform.rotation;\n    }\n\n    public void PublishLaserScan()\n    {\n        // Simulate laser scan by casting rays around the robot\n        int numRays = 360; // 1 degree resolution\n        float[] ranges = new float[numRays];\n        float angleMin = -Mathf.PI;\n        float angleMax = Mathf.PI;\n        float angleIncrement = (angleMax - angleMin) / numRays;\n\n        for (int i = 0; i < numRays; i++)\n        {\n            float angle = angleMin + i * angleIncrement;\n\n            // Calculate ray direction\n            Vector3 rayDirection = new Vector3(\n                Mathf.Cos(angle),\n                0,\n                Mathf.Sin(angle)\n            );\n\n            // Perform raycast\n            RaycastHit hit;\n            if (Physics.Raycast(transform.position, rayDirection, out hit, 30.0f))\n            {\n                ranges[i] = hit.distance;\n            }\n            else\n            {\n                ranges[i] = 30.0f; // Max range\n            }\n        }\n\n        // Create laser scan message\n        var scanMsg = new LaserScanMsg();\n        scanMsg.header = new std_msgs.Header();\n        scanMsg.header.stamp = new builtin_interfaces.Time();\n        scanMsg.header.frame_id = "laser_frame";\n\n        scanMsg.angle_min = angleMin;\n        scanMsg.angle_max = angleMax;\n        scanMsg.angle_increment = angleIncrement;\n        scanMsg.time_increment = 0.0f;\n        scanMsg.scan_time = 0.1f; // 10Hz\n        scanMsg.range_min = 0.1f;\n        scanMsg.range_max = 30.0f;\n        scanMsg.ranges = ranges;\n\n        // Publish scan message\n        ros.Send(scanTopic, scanMsg);\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"unity-c-script-for-camera-integration-camerapublishercs",children:"Unity C# Script for Camera Integration (CameraPublisher.cs)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageGeneration;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Std;\n\npublic class CameraPublisher : MonoBehaviour\n{\n    [SerializeField] private string imageTopic = "/camera/image";\n    [SerializeField] private string depthTopic = "/camera/depth/image_raw";\n    [SerializeField] private string cameraInfoTopic = "/camera/camera_info";\n\n    [SerializeField] private Camera cameraComponent;\n    [SerializeField] private int imageWidth = 640;\n    [SerializeField] private int imageHeight = 480;\n    [SerializeField] private float updateRate = 30.0f; // Hz\n\n    private ROSConnection ros;\n    private RenderTexture renderTexture;\n    private Texture2D texture2D;\n    private float nextUpdateTime;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Create render texture for camera capture\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        cameraComponent.targetTexture = renderTexture;\n\n        // Create texture for reading pixels\n        texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n\n        nextUpdateTime = Time.time;\n    }\n\n    void Update()\n    {\n        if (Time.time >= nextUpdateTime)\n        {\n            PublishCameraData();\n            nextUpdateTime += 1.0f / updateRate;\n        }\n    }\n\n    void PublishCameraData()\n    {\n        // Set the active render texture to read from\n        RenderTexture.active = renderTexture;\n\n        // Read pixels from render texture to texture2D\n        texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        texture2D.Apply();\n\n        // Convert texture to byte array for ROS message\n        byte[] imageData = texture2D.EncodeToJPG();\n\n        // Create and publish image message\n        var imageMsg = new ImageMsg();\n        imageMsg.header = new std_msgs.Header();\n        imageMsg.header.stamp = new builtin_interfaces.Time();\n        imageMsg.header.frame_id = "camera_optical_frame";\n\n        imageMsg.height = (uint)imageHeight;\n        imageMsg.width = (uint)imageWidth;\n        imageMsg.encoding = "rgb8";\n        imageMsg.is_bigendian = 0;\n        imageMsg.step = (uint)(imageWidth * 3); // 3 bytes per pixel (RGB)\n        imageMsg.data = imageData;\n\n        ros.Send(imageTopic, imageMsg);\n\n        // Publish camera info message\n        PublishCameraInfo();\n\n        // Reset active render texture\n        RenderTexture.active = null;\n    }\n\n    void PublishCameraInfo()\n    {\n        // Create camera info message\n        var cameraInfoMsg = new CameraInfoMsg();\n        cameraInfoMsg.header = new std_msgs.Header();\n        cameraInfoMsg.header.stamp = new builtin_interfaces.Time();\n        cameraInfoMsg.header.frame_id = "camera_optical_frame";\n\n        cameraInfoMsg.height = (uint)imageHeight;\n        cameraInfoMsg.width = (uint)imageWidth;\n\n        // Standard camera intrinsics (adjust based on your camera setup)\n        cameraInfoMsg.k = new double[9] {\n            320.0, 0.0, 320.0,   // fx, 0, cx\n            0.0, 320.0, 240.0,   // 0, fy, cy\n            0.0, 0.0, 1.0        // 0, 0, 1\n        };\n\n        // Default distortion coefficients (assuming no distortion)\n        cameraInfoMsg.d = new double[5] { 0.0, 0.0, 0.0, 0.0, 0.0 };\n\n        // Identity rectification matrix\n        cameraInfoMsg.r = new double[9] {\n            1.0, 0.0, 0.0,\n            0.0, 1.0, 0.0,\n            0.0, 0.0, 1.0\n        };\n\n        // Projection matrix\n        cameraInfoMsg.p = new double[12] {\n            320.0, 0.0, 320.0, 0.0,  // fx, 0, cx, Tx\n            0.0, 320.0, 240.0, 0.0,  // 0, fy, cy, Ty\n            0.0, 0.0, 1.0, 0.0       // 0, 0, 1, Tz\n        };\n\n        ros.Send(cameraInfoTopic, cameraInfoMsg);\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"python-ros-2-node-to-interface-with-unity",children:"Python ROS 2 Node to Interface with Unity"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan, Image, CameraInfo\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import Header\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\n\nclass UnityBridgeNode(Node):\n    """\n    ROS 2 node that interfaces with Unity simulation.\n    Demonstrates bidirectional communication with Unity environment.\n    """\n\n    def __init__(self):\n        super().__init__(\'unity_bridge_node\')\n\n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n\n        # Create publishers for Unity commands\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Create subscribers for Unity sensor data\n        self.odom_subscriber = self.create_subscription(\n            Odometry,\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        self.scan_subscriber = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        self.image_subscriber = self.create_subscription(\n            Image,\n            \'/camera/image\',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_subscriber = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # Timer for sending commands\n        self.command_timer = self.create_timer(0.1, self.send_command)  # 10 Hz\n\n        # State variables\n        self.current_position = None\n        self.current_orientation = None\n        self.scan_data = None\n        self.image_data = None\n        self.camera_info = None\n\n        self.get_logger().info(\'Unity bridge node initialized\')\n\n    def odom_callback(self, msg):\n        """Process odometry data from Unity."""\n        self.current_position = (\n            msg.pose.pose.position.x,\n            msg.pose.pose.position.y,\n            msg.pose.pose.position.z\n        )\n\n        self.current_orientation = (\n            msg.pose.pose.orientation.x,\n            msg.pose.pose.orientation.y,\n            msg.pose.pose.orientation.z,\n            msg.pose.pose.orientation.w\n        )\n\n        self.get_logger().info(\n            f\'Unity robot position: {self.current_position}, \'\n            f\'orientation: ({self.current_orientation[2]:.3f})\'\n        )\n\n    def scan_callback(self, msg):\n        """Process laser scan data from Unity."""\n        self.scan_data = msg\n        valid_ranges = [r for r in msg.ranges if 0 < r < float(\'inf\')]\n        if valid_ranges:\n            avg_range = sum(valid_ranges) / len(valid_ranges)\n            self.get_logger().info(f\'Unity scan: {len(valid_ranges)} valid ranges, avg: {avg_range:.2f}m\')\n\n    def image_callback(self, msg):\n        """Process camera image from Unity."""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \'rgb8\')\n\n            # Process image (example: convert to grayscale)\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)\n\n            # Store processed image\n            self.image_data = gray\n\n            # Log image info\n            self.get_logger().info(f\'Unity image received: {cv_image.shape[1]}x{cv_image.shape[0]}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing Unity image: {e}\')\n\n    def camera_info_callback(self, msg):\n        """Process camera info from Unity."""\n        self.camera_info = msg\n        self.get_logger().info(\n            f\'Unity camera info: {msg.width}x{msg.height}, \'\n            f\'fx: {msg.k[0]:.2f}, fy: {msg.k[4]:.2f}\'\n        )\n\n    def send_command(self):\n        """Send velocity commands to Unity."""\n        cmd = Twist()\n\n        # Simple behavior: move forward with occasional turns\n        cmd.linear.x = 0.5  # Forward velocity\n        cmd.angular.z = 0.2 * np.sin(self.get_clock().now().nanoseconds / 1e9)  # Gentle turn\n\n        self.cmd_vel_publisher.publish(cmd)\n\n\ndef main(args=None):\n    """Main function to run the Unity bridge node."""\n    rclpy.init(args=args)\n\n    unity_bridge = UnityBridgeNode()\n\n    try:\n        rclpy.spin(unity_bridge)\n    except KeyboardInterrupt:\n        unity_bridge.get_logger().info(\'Interrupt received, shutting down...\')\n    finally:\n        unity_bridge.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"unity-scene-setup-instructions",children:"Unity Scene Setup Instructions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-markdown",children:"## Unity Scene Setup for Robotics Simulation\n\n### 1. Basic Scene Structure\n- Create a new 3D scene in Unity\n- Add a robot model (imported as FBX/OBJ or created in Unity)\n- Add the RobotController.cs script to the robot GameObject\n- Add a Camera component and attach CameraPublisher.cs script\n\n### 2. Environment Setup\n- Create a ground plane with realistic materials\n- Add lighting (Directional Light for sun, Point Lights for artificial)\n- Create obstacles and objects for testing\n\n### 3. ROS Connection Configuration\n- In RobotController.cs, set the correct topic names\n- Ensure Unity and ROS are on the same network\n- Start ROS TCP Connector in Unity (typically on port 10000)\n\n### 4. Testing the Connection\n- Build and run the Unity scene\n- Start ROS 2 nodes that publish to configured topics\n- Verify bidirectional communication is working\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"[INFO] [unity_bridge_node]: Unity bridge node initialized\n[INFO] [unity_bridge_node]: Unity robot position: (1.25, 0.87, 0.0), orientation: (0.002)\n[INFO] [unity_bridge_node]: Unity scan: 360 valid ranges, avg: 5.23m\n[INFO] [unity_bridge_node]: Unity image received: 640x480\n[INFO] [unity_bridge_node]: Unity camera info: 640x480, fx: 320.00, fy: 320.00\n[INFO] [unity_bridge_node]: Interrupt received, shutting down...\n"})}),"\n",(0,r.jsx)(n.h3,{id:"running-the-example",children:"Running the Example"}),"\n",(0,r.jsx)(n.p,{children:"To run this Unity simulation integration example:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Source ROS 2\nsource /opt/ros/humble/setup.bash\n\n# Terminal 2: Start the Unity bridge node\nros2 run unity_examples unity_bridge_node\n\n# Terminal 3: Monitor topics\nros2 topic echo /odom\nros2 topic echo /scan\nros2 topic echo /camera/image\n\n# In Unity Editor:\n# 1. Import Unity Robotics Hub package\n# 2. Add RobotController.cs and CameraPublisher.cs to your robot GameObject\n# 3. Configure topic names in the inspector\n# 4. Press Play to start simulation\n# 5. Verify ROS communication is working\n"})}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-unity-environment-creation",children:"Exercise 1: Unity Environment Creation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task"}),": Create a complex Unity environment for robotics testing."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Design a 3D environment with multiple rooms and obstacles"}),"\n",(0,r.jsx)(n.li,{children:"Add realistic materials and lighting conditions"}),"\n",(0,r.jsx)(n.li,{children:"Include objects of various shapes, sizes, and textures"}),"\n",(0,r.jsx)(n.li,{children:"Test the environment with your robot simulation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Environment is visually realistic and complex"}),"\n",(0,r.jsx)(n.li,{children:"Materials and lighting are properly configured"}),"\n",(0,r.jsx)(n.li,{children:"Robot can navigate the environment successfully"}),"\n",(0,r.jsx)(n.li,{children:"Environment provides good testing scenarios"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-advanced-sensor-simulation",children:"Exercise 2: Advanced Sensor Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task"}),": Implement additional sensor types in Unity."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Add semantic segmentation camera to identify object types"}),"\n",(0,r.jsx)(n.li,{children:"Implement depth camera for 3D reconstruction"}),"\n",(0,r.jsx)(n.li,{children:"Create IMU sensor simulation in Unity"}),"\n",(0,r.jsx)(n.li,{children:"Integrate all sensors with ROS 2 topics"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All sensor types function correctly in Unity"}),"\n",(0,r.jsx)(n.li,{children:"Data is properly published to ROS 2 topics"}),"\n",(0,r.jsx)(n.li,{children:"Sensor fusion works with Unity-generated data"}),"\n",(0,r.jsx)(n.li,{children:"Data quality is realistic and useful"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-perception-training-data",children:"Exercise 3: Perception Training Data"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task"}),": Use Unity to generate training data for perception tasks."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Implement domain randomization to vary environment properties"}),"\n",(0,r.jsx)(n.li,{children:"Generate synthetic datasets with ground truth annotations"}),"\n",(0,r.jsx)(n.li,{children:"Export data in formats suitable for machine learning"}),"\n",(0,r.jsx)(n.li,{children:"Train a simple model using the synthetic data"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Domain randomization is properly implemented"}),"\n",(0,r.jsx)(n.li,{children:"High-quality synthetic datasets are generated"}),"\n",(0,r.jsx)(n.li,{children:"Training data is properly formatted and labeled"}),"\n",(0,r.jsx)(n.li,{children:"Model trained on synthetic data performs reasonably on real data"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Unity provides high-fidelity graphics simulation that complements physics-focused simulators like Gazebo. We've explored Unity's role in robotics simulation, particularly for perception tasks that require realistic visual rendering. The Unity-ROS integration enables bidirectional communication between Unity's rich 3D environment and ROS 2's robotics ecosystem."}),"\n",(0,r.jsx)(n.p,{children:"We've implemented complete examples showing Unity-ROS communication with C# scripts for robot control and sensor simulation, along with Python nodes for interfacing with Unity from ROS 2. The examples demonstrated how to create photorealistic environments, simulate various sensors, and integrate with existing ROS 2 workflows."}),"\n",(0,r.jsx)(n.p,{children:"Understanding Unity simulation is crucial for Physical AI systems that require advanced perception capabilities. The combination of realistic graphics, advanced rendering features, and ROS integration makes Unity an invaluable tool for training computer vision models, testing perception algorithms, and creating synthetic datasets for machine learning applications."}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"With Module 2 complete, you now have comprehensive knowledge of robotics simulation using both physics-accurate (Gazebo) and high-fidelity graphics (Unity) approaches. You understand how to integrate various sensors and create realistic testing environments."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Module"}),": Module 3: NVIDIA Isaac Simulation"]}),"\n",(0,r.jsx)(n.p,{children:"In Module 3, you'll learn about NVIDIA Isaac, a GPU-accelerated platform for AI-powered robotics. You'll explore Isaac Sim for high-fidelity simulation with RTX rendering, Isaac Gym for massively parallel reinforcement learning, and perception AI tools for vision-based robotics."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);