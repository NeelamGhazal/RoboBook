"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[1806],{943:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4-vla/chapter-2-vla-training","title":"Training VLA Models with Demonstration Data","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-2-vla-training.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-2-vla-training","permalink":"/RoboBook/docs/module-4-vla/chapter-2-vla-training","draft":false,"unlisted":false,"editUrl":"https://github.com/NeelamGhazal/RoboBook/tree/main/docs/module-4-vla/chapter-2-vla-training.md","tags":[],"version":"current","sidebarPosition":18,"frontMatter":{"title":"Training VLA Models with Demonstration Data","sidebar_position":18},"sidebar":"textbookSidebar","previous":{"title":"Chapter 1: VLA Models Introduction","permalink":"/RoboBook/docs/module-4-vla/chapter-1-vla-introduction"},"next":{"title":"Chapter 3: Deploying VLA Models","permalink":"/RoboBook/docs/module-4-vla/chapter-3-vla-deployment"}}');var a=i(4848),s=i(8453);const o={title:"Training VLA Models with Demonstration Data",sidebar_position:18},r="Training VLA Models with Demonstration Data",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Theory",id:"theory",level:2},{value:"Behavioral Cloning",id:"behavioral-cloning",level:3},{value:"Imitation Learning",id:"imitation-learning",level:3},{value:"Data Collection Strategies",id:"data-collection-strategies",level:3},{value:"Dataset Requirements",id:"dataset-requirements",level:3},{value:"Code Example",id:"code-example",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"training-vla-models-with-demonstration-data",children:"Training VLA Models with Demonstration Data"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Explain the importance of demonstration data in VLA model training"}),"\n",(0,a.jsx)(e.li,{children:"Describe different approaches to collecting robotic demonstration data"}),"\n",(0,a.jsx)(e.li,{children:"Understand behavioral cloning and imitation learning techniques"}),"\n",(0,a.jsx)(e.li,{children:"Implement data preprocessing pipelines for VLA training"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate VLA model performance on demonstration datasets"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(e.p,{children:"Before diving into this chapter, you should have:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of VLA model architectures (from Chapter 1)"}),"\n",(0,a.jsx)(e.li,{children:"Basic knowledge of machine learning and neural networks"}),"\n",(0,a.jsx)(e.li,{children:"Experience with PyTorch or TensorFlow"}),"\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 for data collection"}),"\n",(0,a.jsx)(e.li,{children:"Completion of previous modules on simulation and Isaac"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"Training Vision-Language-Action models requires high-quality demonstration data that connects visual observations, language commands, and corresponding robot actions. This demonstration data serves as the foundation for learning the complex mappings between perception, language, and action that enable robots to understand and execute tasks."}),"\n",(0,a.jsx)(e.p,{children:"Demonstration data collection involves recording human experts performing tasks while capturing:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Visual information from robot cameras"}),"\n",(0,a.jsx)(e.li,{children:"Language commands or descriptions"}),"\n",(0,a.jsx)(e.li,{children:"Robot joint angles and actions"}),"\n",(0,a.jsx)(e.li,{children:"Environmental state information"}),"\n",(0,a.jsx)(e.li,{children:"Task success/failure labels"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The quality and diversity of demonstration data significantly impact the final performance of VLA models. Large-scale, diverse datasets enable models to generalize across different objects, environments, and tasks."}),"\n",(0,a.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(e.h3,{id:"behavioral-cloning",children:"Behavioral Cloning"}),"\n",(0,a.jsx)(e.p,{children:"Behavioral cloning is the most common approach for training VLA models on demonstration data. It treats the problem as supervised learning, where the model learns to map observations (vision + language) to actions by mimicking demonstrated behaviors."}),"\n",(0,a.jsx)(e.p,{children:"The loss function typically minimizes the difference between predicted actions and demonstrated actions:"}),"\n",(0,a.jsx)(e.p,{children:"L = ||\u03c0_\u03b8(s) - a_demo||\xb2"}),"\n",(0,a.jsx)(e.p,{children:"Where \u03c0_\u03b8 is the policy network with parameters \u03b8, s is the state (vision + language), and a_demo is the demonstrated action."}),"\n",(0,a.jsx)(e.h3,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,a.jsx)(e.p,{children:"Beyond simple behavioral cloning, imitation learning encompasses various techniques:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"DAgger (Dataset Aggregation)"}),": Addresses distribution shift by iteratively collecting new data from the policy"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Adversarial Imitation Learning"}),": Uses adversarial training to match the expert's behavior distribution"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Offline Reinforcement Learning"}),": Fine-tunes behavioral cloning with RL objectives"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"data-collection-strategies",children:"Data Collection Strategies"}),"\n",(0,a.jsx)(e.p,{children:"Effective VLA training requires diverse and high-quality demonstration data:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Human Demonstrations"}),": Recording human operators controlling robots"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Teleoperation"}),": Using haptic interfaces or joysticks for precise control"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Kinesthetic Teaching"}),": Physically guiding the robot through motions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Virtual Reality"}),": Using VR interfaces for intuitive demonstration"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simulation-to-Real"}),": Collecting data in simulation and transferring to real robots"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"dataset-requirements",children:"Dataset Requirements"}),"\n",(0,a.jsx)(e.p,{children:"High-quality VLA datasets should include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Diverse object categories and arrangements"}),"\n",(0,a.jsx)(e.li,{children:"Various lighting conditions and backgrounds"}),"\n",(0,a.jsx)(e.li,{children:"Multiple language expressions for the same task"}),"\n",(0,a.jsx)(e.li,{children:"Different robot configurations and starting positions"}),"\n",(0,a.jsx)(e.li,{children:"Failure cases and recovery behaviors"}),"\n",(0,a.jsx)(e.li,{children:"Long-horizon tasks with temporal dependencies"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"code-example",children:"Code Example"}),"\n",(0,a.jsx)(e.p,{children:"Here's an example of how to collect and preprocess demonstration data for VLA model training:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport json\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import CLIPTokenizer\n\nclass VLADataset(Dataset):\n    """\n    Dataset class for VLA demonstration data.\n    Assumes data is stored in JSON format with image paths, commands, and actions.\n    """\n\n    def __init__(self, data_dir, transform=None):\n        """\n        Initialize the VLA dataset.\n\n        Args:\n            data_dir: Directory containing demonstration data\n            transform: Image transformations to apply\n        """\n        self.data_dir = data_dir\n        self.transform = transform or transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((224, 224)),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        # Load demonstration data\n        self.demonstrations = []\n        self._load_demonstrations()\n\n    def _load_demonstrations(self):\n        """Load demonstration data from JSON files."""\n        for filename in os.listdir(self.data_dir):\n            if filename.endswith(\'.json\'):\n                with open(os.path.join(self.data_dir, filename), \'r\') as f:\n                    demo_data = json.load(f)\n                    self.demonstrations.extend(demo_data[\'episodes\'])\n\n    def __len__(self):\n        return len(self.demonstrations)\n\n    def __getitem__(self, idx):\n        """\n        Get a single demonstration sample.\n\n        Args:\n            idx: Index of the demonstration\n\n        Returns:\n            Dictionary containing image, command, and action\n        """\n        episode = self.demonstrations[idx]\n\n        # Load image\n        image_path = os.path.join(self.data_dir, episode[\'image_path\'])\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Process command\n        command = episode[\'command\']\n\n        # Process action\n        action = torch.tensor(episode[\'action\'], dtype=torch.float32)\n\n        return {\n            \'image\': image,\n            \'command\': command,\n            \'action\': action\n        }\n\nclass VLADataCollector:\n    """\n    Class for collecting VLA demonstration data from ROS 2 topics.\n    """\n\n    def __init__(self, save_dir="vla_demonstrations"):\n        self.save_dir = save_dir\n        self.episode_data = []\n        self.current_episode = []\n\n        # Create save directory\n        os.makedirs(save_dir, exist_ok=True)\n\n        # Initialize ROS 2 subscriptions\n        # In practice, you would subscribe to camera, joint state, and command topics\n        # self.camera_sub = rospy.Subscriber(\'/camera/image_raw\', Image, self.image_callback)\n        # self.joint_sub = rospy.Subscriber(\'/joint_states\', JointState, self.joint_callback)\n        # self.command_sub = rospy.Subscriber(\'/command\', String, self.command_callback)\n\n    def start_episode(self):\n        """Start a new demonstration episode."""\n        self.current_episode = []\n        print("Starting new demonstration episode...")\n\n    def record_step(self, image, joints, command, action):\n        """\n        Record a single step of the demonstration.\n\n        Args:\n            image: Current camera image\n            joints: Current joint states\n            command: Language command\n            action: Action taken (next joint positions or velocity)\n        """\n        step_data = {\n            \'image_path\': f"step_{len(self.current_episode)}.jpg",\n            \'joints\': joints.tolist() if isinstance(joints, np.ndarray) else joints,\n            \'command\': command,\n            \'action\': action.tolist() if isinstance(action, np.ndarray) else action,\n            \'timestamp\': len(self.current_episode)  # Relative timestamp within episode\n        }\n\n        # Save image\n        image_filename = os.path.join(self.save_dir, step_data[\'image_path\'])\n        cv2.imwrite(image_filename, image)\n\n        self.current_episode.append(step_data)\n\n    def end_episode(self, success=True):\n        """End the current demonstration episode."""\n        episode_data = {\n            \'steps\': self.current_episode,\n            \'success\': success,\n            \'length\': len(self.current_episode)\n        }\n\n        # Save episode data\n        episode_id = len(self.episode_data)\n        episode_filename = os.path.join(self.save_dir, f"episode_{episode_id}.json")\n\n        with open(episode_filename, \'w\') as f:\n            json.dump(episode_data, f, indent=2)\n\n        self.episode_data.append(episode_data)\n        print(f"Episode saved: {episode_filename}")\n\n    def collect_demonstration(self, task_description):\n        """\n        Collect a full demonstration for a specific task.\n\n        Args:\n            task_description: Natural language description of the task\n        """\n        print(f"Collecting demonstration for: {task_description}")\n\n        self.start_episode()\n\n        # In practice, this would involve:\n        # 1. Waiting for user to start task execution\n        # 2. Continuously recording state-action pairs\n        # 3. Detecting task completion\n        # 4. Saving the complete episode\n\n        # Simulate some steps for demonstration\n        for i in range(10):  # Simulate 10 steps\n            # Simulated data (in practice, this comes from ROS 2 topics)\n            dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n            dummy_joints = np.random.rand(7).astype(np.float32)\n            dummy_action = np.random.rand(7).astype(np.float32)\n\n            self.record_step(dummy_image, dummy_joints, task_description, dummy_action)\n\n        self.end_episode(success=True)\n\ndef train_vla_model(model, dataset, epochs=10, batch_size=32, lr=1e-4):\n    """\n    Train a VLA model using behavioral cloning.\n\n    Args:\n        model: VLA model to train\n        dataset: VLA dataset\n        epochs: Number of training epochs\n        batch_size: Training batch size\n        lr: Learning rate\n    """\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    model.train()\n\n    for epoch in range(epochs):\n        total_loss = 0.0\n\n        for batch in dataloader:\n            images = batch[\'image\']\n            commands = batch[\'command\']\n            actions = batch[\'action\']\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            predicted_actions = model(images, commands)\n\n            # Calculate loss\n            loss = criterion(predicted_actions, actions)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dataloader)\n        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")\n\n# Example usage\ndef example_usage():\n    """\n    Example of how to use the VLA data collection and training pipeline.\n    """\n    # Initialize data collector\n    collector = VLADataCollector()\n\n    # Collect demonstrations for different tasks\n    tasks = [\n        "Pick up the red cup and place it on the table",\n        "Move the blue box to the left side",\n        "Open the drawer and take out the object"\n    ]\n\n    for task in tasks:\n        collector.collect_demonstration(task)\n\n    # Create dataset from collected demonstrations\n    dataset = VLADataset(collector.save_dir)\n\n    # Initialize model (assuming we have a VLA model class)\n    # model = VLAModel(action_dim=7)  # 7-DOF robot arm\n\n    # Train the model\n    # train_vla_model(model, dataset)\n\n    print(f"Collected {len(dataset)} demonstration samples")\n\nif __name__ == "__main__":\n    example_usage()\n'})}),"\n",(0,a.jsx)(e.p,{children:"This code example demonstrates a complete pipeline for collecting demonstration data for VLA models and training them using behavioral cloning. The code includes data collection utilities, dataset classes, and training functions."}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Data Collection Enhancement"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Extend the VLADataCollector class to support multiple camera views"}),"\n",(0,a.jsx)(e.li,{children:"Add support for different sensor modalities (LiDAR, IMU, etc.)"}),"\n",(0,a.jsx)(e.li,{children:"Implement automatic episode segmentation based on motion detection"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Advanced Training Techniques"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement DAgger algorithm for iterative improvement"}),"\n",(0,a.jsx)(e.li,{children:"Add data augmentation techniques for vision and language inputs"}),"\n",(0,a.jsx)(e.li,{children:"Implement curriculum learning for progressive skill acquisition"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Evaluation Metrics"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design evaluation metrics for VLA model performance"}),"\n",(0,a.jsx)(e.li,{children:"Implement success rate calculation for different task types"}),"\n",(0,a.jsx)(e.li,{children:"Create visualization tools for analyzing model predictions vs. ground truth"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Training VLA models with demonstration data requires careful consideration of data collection, preprocessing, and training methodologies. Behavioral cloning provides a solid foundation for learning from expert demonstrations, while more advanced techniques like DAgger and adversarial imitation learning can address distribution shift and improve performance."}),"\n",(0,a.jsx)(e.p,{children:"The quality and diversity of demonstration data significantly impact the final model performance. Effective data collection strategies should capture various scenarios, objects, and environmental conditions to enable robust generalization."}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"In the next chapter, we'll explore how to deploy VLA models on real robots using ROS 2, including inference optimization, safety considerations, and integration with existing robotic systems."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);