"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9912],{1188:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2-simulation/chapter-3-imu-integration","title":"IMU Sensor Integration","description":"Learning Objectives","source":"@site/docs/module-2-simulation/chapter-3-imu-integration.md","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/chapter-3-imu-integration","permalink":"/RoboBook/docs/module-2-simulation/chapter-3-imu-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/NeelamGhazal/RoboBook/tree/main/docs/module-2-simulation/chapter-3-imu-integration.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"IMU Sensor Integration","sidebar_position":9},"sidebar":"textbookSidebar","previous":{"title":"Chapter 2: LiDAR Integration","permalink":"/RoboBook/docs/module-2-simulation/chapter-2-lidar-integration"},"next":{"title":"Chapter 4: Depth Camera Integration","permalink":"/RoboBook/docs/module-2-simulation/chapter-4-depth-camera"}}');var t=i(4848),o=i(8453);const r={title:"IMU Sensor Integration",sidebar_position:9},a="IMU Sensor Integration",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Knowledge Prerequisites",id:"knowledge-prerequisites",level:3},{value:"Software Prerequisites",id:"software-prerequisites",level:3},{value:"Installation Verification",id:"installation-verification",level:3},{value:"Introduction",id:"introduction",level:2},{value:"Theory",id:"theory",level:2},{value:"IMU Principles",id:"imu-principles",level:3},{value:"IMU in Robotics Applications",id:"imu-in-robotics-applications",level:3},{value:"Gazebo IMU Simulation",id:"gazebo-imu-simulation",level:3},{value:"Sensor Fusion for Orientation",id:"sensor-fusion-for-orientation",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"URDF with IMU Sensor (robot_with_imu.urdf.xacro)",id:"urdf-with-imu-sensor-robot_with_imuurdfxacro",level:3},{value:"IMU Data Processing Node",id:"imu-data-processing-node",level:3},{value:"Sensor Fusion Node (IMU + Odometry)",id:"sensor-fusion-node-imu--odometry",level:3},{value:"Running the Example",id:"running-the-example",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: IMU Parameter Tuning",id:"exercise-1-imu-parameter-tuning",level:3},{value:"Exercise 2: Advanced Sensor Fusion",id:"exercise-2-advanced-sensor-fusion",level:3},{value:"Exercise 3: IMU-Based Control",id:"exercise-3-imu-based-control",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"imu-sensor-integration",children:"IMU Sensor Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Explain the principles of IMU (Inertial Measurement Unit) sensors and their role in robotics"}),"\n",(0,t.jsx)(n.li,{children:"Implement IMU sensor simulation in Gazebo with realistic parameters"}),"\n",(0,t.jsx)(n.li,{children:"Process IMU data using the sensor_msgs/Imu message type"}),"\n",(0,t.jsx)(n.li,{children:"Apply sensor fusion techniques to estimate robot orientation and motion"}),"\n",(0,t.jsx)(n.li,{children:"Integrate IMU data with robot localization and control systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.h3,{id:"knowledge-prerequisites",children:"Knowledge Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Fundamentals"}),": Understanding of nodes, topics, and message types (Module 1)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"URDF Robot Description"}),": Understanding of robot models and sensor integration (Module 1, Chapter 5)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gazebo Simulation Basics"}),": Understanding of physics simulation concepts (Module 2, Chapter 1)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LiDAR Integration"}),": Understanding of sensor simulation and processing (Module 2, Chapter 2)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mathematics"}),": Basic understanding of 3D rotations, quaternions, and linear algebra"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"software-prerequisites",children:"Software Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operating System"}),": Ubuntu 22.04 LTS with ROS 2 Humble Hawksbill installed"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation Software"}),": Gazebo Garden (or Fortress) with ROS 2 integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Python"}),": Version 3.10 or higher"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visualization Tools"}),": RViz2 for IMU data visualization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mathematical Libraries"}),": NumPy for numerical computations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Terminal"}),": Bash shell access"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"installation-verification",children:"Installation Verification"}),"\n",(0,t.jsx)(n.p,{children:"Verify your IMU simulation environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check available IMU topics\nros2 topic list | grep imu\n\n# Check IMU message type\nros2 interface show sensor_msgs/msg/Imu\n\n# Verify Gazebo IMU plugins\ngz topic -l | grep imu\n"})}),"\n",(0,t.jsx)(n.p,{children:"Expected output: Available topics, message definitions, and plugins related to IMU sensors."}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"In the previous chapters, we explored LiDAR sensors for environmental perception. Now we'll focus on IMU (Inertial Measurement Unit) sensors, which provide critical information about a robot's own motion and orientation. IMUs are essential for robot navigation, stabilization, and control, providing real-time measurements of acceleration, angular velocity, and orientation that enable robots to understand their own movement in 3D space."}),"\n",(0,t.jsx)(n.p,{children:"Think of an IMU as a robot's \"inner ear\" - just as humans use their vestibular system to sense head orientation, rotation, and linear acceleration, robots use IMUs to understand their own motion. While LiDAR tells a robot where things are around it, IMUs tell the robot where it is and how it's moving. This self-awareness is fundamental to autonomous navigation, especially in GPS-denied environments where external positioning systems aren't available."}),"\n",(0,t.jsx)(n.p,{children:"In Physical AI systems, IMUs serve multiple purposes: they provide orientation estimates for robot stabilization, enable dead-reckoning navigation when other sensors fail, and contribute to sensor fusion algorithms that combine multiple data sources for robust state estimation. Modern robots typically use IMUs as part of a complete navigation solution, combining IMU data with wheel encoders, visual odometry, and other sensors to maintain accurate position estimates."}),"\n",(0,t.jsx)(n.p,{children:"In this chapter, we'll explore how to simulate IMU sensors in Gazebo, configure realistic sensor parameters, and process the resulting data in ROS 2. We'll learn to integrate IMU data into robot localization systems and understand how to fuse multiple sensor sources for robust state estimation."}),"\n",(0,t.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,t.jsx)(n.h3,{id:"imu-principles",children:"IMU Principles"}),"\n",(0,t.jsx)(n.p,{children:"An IMU typically contains three types of sensors:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration along three axes (x, y, z)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity around three axes (roll, pitch, yaw)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Magnetometer"}),": Measures magnetic field strength (provides absolute heading reference)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The sensor_msgs/Imu message type in ROS 2 combines all this information:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"orientation[]"}),": Quaternion representing orientation (x, y, z, w)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"orientation_covariance[]"}),": Covariance matrix for orientation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"angular_velocity[]"}),": Angular velocity vector (x, y, z)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"angular_velocity_covariance[]"}),": Covariance matrix for angular velocity"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"linear_acceleration[]"}),": Linear acceleration vector (x, y, z)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"linear_acceleration_covariance[]"}),": Covariance matrix for linear acceleration"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"imu-in-robotics-applications",children:"IMU in Robotics Applications"}),"\n",(0,t.jsx)(n.p,{children:"IMUs enable several critical robot capabilities:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Attitude Estimation"}),": Determine robot orientation relative to gravity"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Motion Detection"}),": Sense robot movement and direction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stabilization"}),": Maintain balance in legged robots or steady platforms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dead Reckoning"}),": Estimate position based on motion integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Combine with other sensors for robust localization"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"gazebo-imu-simulation",children:"Gazebo IMU Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Gazebo simulates IMU sensors using physics-based motion tracking:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'%%{title: "Gazebo IMU Simulation Pipeline"}%%\ngraph TD\n    A[IMU Sensor Model] --\x3e B[Physics Engine Motion Tracking]\n    B --\x3e C[Linear Acceleration Calculation]\n    C --\x3e D[Angular Velocity Calculation]\n    D --\x3e E[Quaternion Orientation]\n    E --\x3e F[Imu Message]\n    F --\x3e G[ROS 2 Topic /imu/data]\n    G --\x3e H[State Estimation]\n'})}),"\n",(0,t.jsx)(n.p,{children:"The simulation accurately models:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Motion-based measurements (no external reference needed)"}),"\n",(0,t.jsx)(n.li,{children:"Sensor noise characteristics"}),"\n",(0,t.jsx)(n.li,{children:"Bias and drift effects"}),"\n",(0,t.jsx)(n.li,{children:"Coordinate frame transformations"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"sensor-fusion-for-orientation",children:"Sensor Fusion for Orientation"}),"\n",(0,t.jsx)(n.p,{children:"Raw IMU data has limitations:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Accelerometers are affected by motion-induced acceleration"}),"\n",(0,t.jsx)(n.li,{children:"Gyroscopes drift over time due to integration errors"}),"\n",(0,t.jsx)(n.li,{children:"Magnetometers can be affected by magnetic interference"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Sensor fusion algorithms combine IMU data with other sources to estimate orientation more accurately:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complementary Filter"}),": Combines accelerometer/magnetometer (slow, absolute) with gyroscope (fast, relative)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kalman Filter"}),": Statistically optimal fusion with uncertainty modeling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Madgwick Filter"}),": Computationally efficient for real-time applications"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,t.jsx)(n.p,{children:"Let's implement a complete IMU sensor integration example:"}),"\n",(0,t.jsx)(n.h3,{id:"urdf-with-imu-sensor-robot_with_imuurdfxacro",children:"URDF with IMU Sensor (robot_with_imu.urdf.xacro)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="robot_with_imu">\n\n  \x3c!-- Constants --\x3e\n  <xacro:property name="M_PI" value="3.1415926535897931" />\n\n  \x3c!-- Robot base properties --\x3e\n  <xacro:property name="base_width" value="0.4" />\n  <xacro:property name="base_length" value="0.6" />\n  <xacro:property name="base_height" value="0.2" />\n  <xacro:property name="base_mass" value="10.0" />\n\n  \x3c!-- Wheel properties --\x3e\n  <xacro:property name="wheel_radius" value="0.1" />\n  <xacro:property name="wheel_width" value="0.05" />\n  <xacro:property name="wheel_mass" value="1.0" />\n  <xacro:property name="wheel_offset_x" value="0.2" />\n  <xacro:property name="wheel_offset_y" value="0.25" />\n  <xacro:property name="wheel_offset_z" value="-0.05" />\n\n  \x3c!-- IMU properties --\x3e\n  <xacro:property name="imu_size" value="0.02" />\n  <xacro:property name="imu_mass" value="0.01" />\n\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${base_length} ${base_width} ${base_height}"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${base_length} ${base_width} ${base_height}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${base_mass}"/>\n      <inertia\n        ixx="${base_mass/12.0 * (base_width*base_width + base_height*base_height)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${base_mass/12.0 * (base_length*base_length + base_height*base_height)}"\n        iyz="0.0"\n        izz="${base_mass/12.0 * (base_length*base_length + base_width*base_width)}" />\n    </inertial>\n  </link>\n\n  \x3c!-- Left wheel --\x3e\n  <link name="left_wheel">\n    <visual>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${wheel_mass}"/>\n      <inertia\n        ixx="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        iyz="0.0"\n        izz="${wheel_mass/2.0 * wheel_radius*wheel_radius}" />\n    </inertial>\n  </link>\n\n  <joint name="left_wheel_joint" type="continuous">\n    <origin xyz="${wheel_offset_x} ${wheel_offset_y} ${wheel_offset_z}" rpy="0 0 0"/>\n    <parent link="base_link"/>\n    <child link="left_wheel"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  \x3c!-- Right wheel --\x3e\n  <link name="right_wheel">\n    <visual>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>\n      <geometry>\n        <cylinder radius="${wheel_radius}" length="${wheel_width}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${wheel_mass}"/>\n      <inertia\n        ixx="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${wheel_mass/12.0 * (3*wheel_radius*wheel_radius + wheel_width*wheel_width)}"\n        iyz="0.0"\n        izz="${wheel_mass/2.0 * wheel_radius*wheel_radius}" />\n    </inertial>\n  </link>\n\n  <joint name="right_wheel_joint" type="continuous">\n    <origin xyz="${wheel_offset_x} ${-wheel_offset_y} ${wheel_offset_z}" rpy="0 0 0"/>\n    <parent link="base_link"/>\n    <child link="right_wheel"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  \x3c!-- IMU sensor --\x3e\n  <link name="imu_link">\n    <visual>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${imu_size} ${imu_size} ${imu_size}"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <geometry>\n        <box size="${imu_size} ${imu_size} ${imu_size}"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <origin xyz="0 0 0" rpy="0 0 0"/>\n      <mass value="${imu_mass}"/>\n      <inertia\n        ixx="${imu_mass/12.0 * (imu_size*imu_size + imu_size*imu_size)}"\n        ixy="0.0"\n        ixz="0.0"\n        iyy="${imu_mass/12.0 * (imu_size*imu_size + imu_size*imu_size)}"\n        iyz="0.0"\n        izz="${imu_mass/12.0 * (imu_size*imu_size + imu_size*imu_size)}" />\n    </inertial>\n  </link>\n\n  <joint name="imu_joint" type="fixed">\n    <origin xyz="0 0 ${base_height/2 + imu_size/2 + 0.01}" rpy="0 0 0"/>\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n  </joint>\n\n  \x3c!-- Gazebo plugin for IMU sensor --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <visualize>true</visualize>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\n        <ros>\n          <namespace>/</namespace>\n          <remapping>~/out:=imu/data</remapping>\n        </ros>\n        <update_rate>100</update_rate>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,t.jsx)(n.h3,{id:"imu-data-processing-node",children:"IMU Data Processing Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom std_msgs.msg import Header\nfrom geometry_msgs.msg import Vector3\nimport math\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\nclass ImuProcessor(Node):\n    """\n    Node that processes IMU data to estimate orientation and detect motion.\n    Demonstrates common IMU processing techniques in ROS 2.\n    """\n\n    def __init__(self):\n        super().__init__(\'imu_processor\')\n\n        # Create subscriber for IMU data\n        self.imu_subscriber = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Create publisher for processed data\n        self.orientation_publisher = self.create_publisher(\n            Imu,\n            \'/imu/orientation\',\n            10\n        )\n\n        # Initialize processing parameters\n        self.gravity_threshold = 0.1  # m/s\xb2 for static detection\n        self.angular_velocity_threshold = 0.05  # rad/s for motion detection\n\n        # For orientation estimation (simple complementary filter)\n        self.orientation_estimate = [0.0, 0.0, 0.0, 1.0]  # x, y, z, w\n        self.last_time = None\n\n        self.get_logger().info(\'IMU processor initialized\')\n\n    def imu_callback(self, msg):\n        """\n        Process incoming IMU data.\n        """\n        current_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n\n        # Extract raw measurements\n        accel = np.array([msg.linear_acceleration.x,\n                         msg.linear_acceleration.y,\n                         msg.linear_acceleration.z])\n        gyro = np.array([msg.angular_velocity.x,\n                        msg.angular_velocity.y,\n                        msg.angular_velocity.z])\n\n        # Calculate magnitude of acceleration (excluding gravity)\n        accel_magnitude = np.linalg.norm(accel)\n        gravity_magnitude = 9.81  # nominal gravity\n        linear_accel_magnitude = abs(accel_magnitude - gravity_magnitude)\n\n        # Calculate angular velocity magnitude\n        gyro_magnitude = np.linalg.norm(gyro)\n\n        # Detect motion state\n        is_static = (linear_accel_magnitude < self.gravity_threshold and\n                    gyro_magnitude < self.angular_velocity_threshold)\n\n        # Log motion state\n        if is_static:\n            self.get_logger().info(\'Robot is static\')\n        else:\n            self.get_logger().info(\n                f\'Motion detected - Accel mag: {linear_accel_magnitude:.3f}, \'\n                f\'Gyro mag: {gyro_magnitude:.3f}\'\n            )\n\n        # Simple orientation estimation using accelerometer for roll/pitch\n        # (magnetometer would be needed for absolute yaw)\n        if linear_accel_magnitude < self.gravity_threshold:\n            # Use accelerometer to estimate roll and pitch when static\n            roll = math.atan2(accel[1], math.sqrt(accel[0]**2 + accel[2]**2))\n            pitch = math.atan2(-accel[0], math.sqrt(accel[1]**2 + accel[2]**2))\n\n            # Convert to quaternion (simplified - only roll/pitch)\n            cy = math.cos(0.0)  # yaw assumed 0 without magnetometer\n            sy = math.sin(0.0)\n            cp = math.cos(pitch)\n            sp = math.sin(pitch)\n            cr = math.cos(roll)\n            sr = math.sin(roll)\n\n            quat = [\n                sr * cp * cy - cr * sp * sy,  # x\n                cr * sp * cy + sr * cp * sy,  # y\n                cr * cp * sy - sr * sp * cy,  # z\n                cr * cp * cy + sr * sp * sy   # w\n            ]\n\n            # Publish estimated orientation\n            orientation_msg = Imu()\n            orientation_msg.header = Header()\n            orientation_msg.header.stamp = msg.header.stamp\n            orientation_msg.header.frame_id = msg.header.frame_id\n\n            # Copy orientation from estimate\n            orientation_msg.orientation.x = quat[0]\n            orientation_msg.orientation.y = quat[1]\n            orientation_msg.orientation.z = quat[2]\n            orientation_msg.orientation.w = quat[3]\n\n            # Copy angular velocity and linear acceleration\n            orientation_msg.angular_velocity = msg.angular_velocity\n            orientation_msg.linear_acceleration = msg.linear_acceleration\n\n            # Set covariance to indicate uncertainty\n            orientation_msg.orientation_covariance = [0.01, 0, 0, 0, 0.01, 0, 0, 0, 0.01]\n\n            self.orientation_publisher.publish(orientation_msg)\n\n    def complementary_filter(self, accel, gyro, dt):\n        """\n        Simple complementary filter for orientation estimation.\n        Combines gyroscope integration with accelerometer correction.\n        """\n        # Convert gyro readings to quaternion derivative\n        # (simplified - full implementation would use quaternion math)\n        gyro_norm = np.linalg.norm(gyro)\n        if gyro_norm > 0:\n            # Normalize gyro vector\n            gyro_unit = gyro / gyro_norm\n            # Calculate rotation quaternion from angular velocity\n            angle = gyro_norm * dt\n            s = math.sin(angle / 2)\n            w = math.cos(angle / 2)\n            dq = np.array([s * gyro_unit[0], s * gyro_unit[1], s * gyro_unit[2], w])\n        else:\n            dq = np.array([0.0, 0.0, 0.0, 1.0])\n\n        # Apply rotation to current estimate\n        # (simplified quaternion multiplication)\n        q = np.array(self.orientation_estimate)\n        # In a real implementation, proper quaternion multiplication would be used\n        new_q = q  # Placeholder - full implementation needed for production\n\n        return new_q.tolist()\n\n\ndef main(args=None):\n    """Main function to run the IMU processor."""\n    rclpy.init(args=args)\n\n    processor = ImuProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info(\'Interrupt received, shutting down...\')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"sensor-fusion-node-imu--odometry",children:"Sensor Fusion Node (IMU + Odometry)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import Pose, Twist, Point, Quaternion\nimport math\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\nclass ImuOdometryFusion(Node):\n    """\n    Node that fuses IMU and odometry data for improved state estimation.\n    Demonstrates sensor fusion techniques in ROS 2.\n    """\n\n    def __init__(self):\n        super().__init__(\'imu_odom_fusion\')\n\n        # Create subscribers\n        self.imu_subscriber = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.odom_subscriber = self.create_subscription(\n            Odometry,\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        # Create publisher for fused data\n        self.fused_publisher = self.create_publisher(\n            Odometry,\n            \'/odom_fused\',\n            10\n        )\n\n        # Initialize state variables\n        self.current_orientation = [0.0, 0.0, 0.0, 1.0]  # x, y, z, w\n        self.current_position = [0.0, 0.0, 0.0]  # x, y, z\n        self.current_velocity = [0.0, 0.0, 0.0]  # x, y, z\n        self.last_time = None\n\n        # Fusion parameters\n        self.imu_weight = 0.7  # How much to trust IMU vs odometry\n        self.odom_weight = 0.3\n\n        self.get_logger().info(\'IMU-Odometry fusion node initialized\')\n\n    def imu_callback(self, msg):\n        """\n        Process IMU data and update orientation estimate.\n        """\n        # Extract orientation from IMU (if available)\n        if msg.orientation.w != 0.0:\n            self.current_orientation = [\n                msg.orientation.x,\n                msg.orientation.y,\n                msg.orientation.z,\n                msg.orientation.w\n            ]\n\n    def odom_callback(self, msg):\n        """\n        Process odometry data and fuse with IMU.\n        """\n        current_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n\n        # Extract position and velocity from odometry\n        pos = [\n            msg.pose.pose.position.x,\n            msg.pose.pose.position.y,\n            msg.pose.pose.position.z\n        ]\n\n        vel = [\n            msg.twist.twist.linear.x,\n            msg.twist.twist.linear.y,\n            msg.twist.twist.linear.z\n        ]\n\n        # Create fused odometry message\n        fused_odom = Odometry()\n        fused_odom.header = msg.header\n        fused_odom.header.frame_id = \'odom\'\n        fused_odom.child_frame_id = \'base_link\'\n\n        # Use position from odometry (typically more reliable for position)\n        fused_odom.pose.pose.position = Point(x=pos[0], y=pos[1], z=pos[2])\n\n        # Use orientation from IMU (typically more reliable for orientation)\n        fused_odom.pose.pose.orientation = Quaternion(\n            x=self.current_orientation[0],\n            y=self.current_orientation[1],\n            z=self.current_orientation[2],\n            w=self.current_orientation[3]\n        )\n\n        # Use velocity from odometry\n        fused_odom.twist.twist.linear = Twist(\n            x=vel[0],\n            y=vel[1],\n            z=vel[2]\n        ).linear\n\n        # Copy angular velocity from IMU if available\n        fused_odom.twist.twist.angular = msg.twist.twist.angular\n\n        # Set covariance to reflect fusion uncertainty\n        fused_odom.pose.covariance = [0.1] * 36  # Simplified - real implementation would be more detailed\n        fused_odom.twist.covariance = [0.1] * 36\n\n        # Publish fused data\n        self.fused_publisher.publish(fused_odom)\n\n        self.get_logger().info(\n            f\'Fused odometry: pos=({pos[0]:.2f}, {pos[1]:.2f}), \'\n            f\'orientation=({self.current_orientation[2]:.3f})\'  # z component as simple orientation indicator\n        )\n\n\ndef main(args=None):\n    """Main function to run the sensor fusion node."""\n    rclpy.init(args=args)\n\n    fusion_node = ImuOdometryFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        fusion_node.get_logger().info(\'Interrupt received, shutting down...\')\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[INFO] [imu_processor]: IMU processor initialized\n[INFO] [imu_processor]: Motion detected - Accel mag: 0.150, Gyro mag: 0.080\n[INFO] [imu_processor]: Robot is static\n[INFO] [imu_odom_fusion]: Fused odometry: pos=(1.25, 0.87), orientation=(0.002)\n[INFO] [imu_odom_fusion]: Interrupt received, shutting down...\n"})}),"\n",(0,t.jsx)(n.h3,{id:"running-the-example",children:"Running the Example"}),"\n",(0,t.jsx)(n.p,{children:"To run this IMU integration example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start Gazebo with robot model\nsource /opt/ros/humble/setup.bash\ngz sim -r simple_world.sdf\n\n# Terminal 2: Spawn the robot with IMU in Gazebo\nsource /opt/ros/humble/setup.bash\nros2 run gazebo_ros spawn_entity.py -topic robot_description -entity my_robot\n\n# Terminal 3: Run the IMU processor\nsource /opt/ros/humble/setup.bash\nros2 run my_package imu_processor\n\n# Terminal 4: Run the sensor fusion node\nsource /opt/ros/humble/setup.bash\nros2 run my_package imu_odom_fusion\n\n# Terminal 5: Monitor IMU data\nsource /opt/ros/humble/setup.bash\nros2 topic echo /imu/data\n\n# Terminal 6: Visualize in RViz2\nsource /opt/ros/humble/setup.bash\nrviz2\n# In RViz2: Add TF display to see orientation changes, and add plots for IMU data\n"})}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-imu-parameter-tuning",children:"Exercise 1: IMU Parameter Tuning"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Experiment with different IMU sensor parameters in Gazebo."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Modify the URDF to change IMU noise characteristics and update rates"}),"\n",(0,t.jsx)(n.li,{children:"Compare sensor performance with different noise levels"}),"\n",(0,t.jsx)(n.li,{children:"Document how noise affects orientation estimation accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Test the impact of different update rates on system performance"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Different IMU configurations are properly implemented"}),"\n",(0,t.jsx)(n.li,{children:"Performance differences are documented"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of noise vs. update rate trade-offs"}),"\n",(0,t.jsx)(n.li,{children:"Optimal settings for specific applications are identified"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-advanced-sensor-fusion",children:"Exercise 2: Advanced Sensor Fusion"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Implement a more sophisticated sensor fusion algorithm."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create a Kalman filter to combine IMU and odometry data"}),"\n",(0,t.jsx)(n.li,{children:"Implement proper uncertainty propagation"}),"\n",(0,t.jsx)(n.li,{children:"Compare fusion results with individual sensor outputs"}),"\n",(0,t.jsx)(n.li,{children:"Test the algorithm under various motion conditions"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Kalman filter properly implemented and tuned"}),"\n",(0,t.jsx)(n.li,{children:"Fusion results are more accurate than individual sensors"}),"\n",(0,t.jsx)(n.li,{children:"Uncertainty estimates are reasonable"}),"\n",(0,t.jsx)(n.li,{children:"Algorithm performs well under different conditions"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-imu-based-control",children:"Exercise 3: IMU-Based Control"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Use IMU data for robot stabilization or control."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a simple feedback controller using IMU orientation"}),"\n",(0,t.jsx)(n.li,{children:"Create a balancing robot that uses IMU to maintain upright position"}),"\n",(0,t.jsx)(n.li,{children:"Test the controller with different gain values"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate stability and performance metrics"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"IMU-based controller functions correctly"}),"\n",(0,t.jsx)(n.li,{children:"Robot maintains stability using IMU feedback"}),"\n",(0,t.jsx)(n.li,{children:"Control parameters are properly tuned"}),"\n",(0,t.jsx)(n.li,{children:"System demonstrates effective use of IMU data"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"IMU sensors are fundamental to robotics, providing critical information about robot motion, orientation, and acceleration. We've explored how to integrate IMU sensors in Gazebo simulation, configure realistic parameters with appropriate noise models, and process the resulting data in ROS 2. The sensor_msgs/Imu message type provides a standardized interface for IMU data processing across different platforms and applications."}),"\n",(0,t.jsx)(n.p,{children:"We've implemented complete examples showing IMU integration in URDF models, data processing nodes for orientation estimation, and sensor fusion techniques that combine IMU data with other sources. The examples demonstrated how to extract meaningful orientation information from raw IMU measurements and use it for robot state estimation."}),"\n",(0,t.jsx)(n.p,{children:"Understanding IMU integration is crucial for Physical AI systems that require accurate self-awareness for navigation, stabilization, and control. The combination of acceleration, angular velocity, and orientation measurements enables robots to operate effectively even in challenging environments where external positioning systems aren't available."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Now that you understand IMU sensor integration, the next chapter explores depth camera sensors for 3D perception. You'll learn how to simulate and process RGB-D camera data for environment understanding and mapping."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next Chapter"}),": Module 2, Chapter 4: Depth Camera Integration"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);